version: 2

defaults: &defaults
  docker:
    - image: palantirtechnologies/circle-spark-base:0.2.3
  resource_class: xlarge
  environment: &defaults-environment
    TERM: dumb
    SBT_OPTS: "-Xmx4G -XX:MaxPermSize=2G"


test-defaults: &test-defaults
  <<: *defaults
  environment:
    <<: *defaults-environment
    CIRCLE_TEST_REPORTS: /tmp/circle-test-reports


all-branches-and-tags: &all-branches-and-tags
  filters:
    # run on all branches and tags
    tags:
      only: /.*/

deployable-branches-and-tags: &deployable-branches-and-tags
  filters:
    tags:
      only: /[0-9]+(?:\.[0-9]+){2,}-palantir\.[0-9]+(?:\.[0-9]+)*/
    branches:
      only: master


# Step templates

step_templates:
  create-pom-checksum-file: &create-pom-checksum-file
    run:
      name: Create file of pom.xml checksums
      command: find . -maxdepth 4 -name 'pom.xml' -exec shasum {} \; > /tmp/pom_checksums.txt
  cache-build-tools: &cache-build-tools
    # Upstream's caching scheme: https://github.com/apache/spark/blob/eb5558ed35d51215de2ab12156e477cd7130a66a/.github/workflows/build_and_test.yml#L183-L194
    # Only save_cache in build-maven job, which downloads both mvn and sbt tools
    save_cache:
      key: build-tool-cache-v2-{{ .Branch }}-{{ checksum "/tmp/pom_checksums.txt" }}-{{ checksum "project/build.properties" }}
      paths:
        - build/apache-maven-*
        - build/zinc-*
        - build/scala-*
        - build/*.jar
  restore-build-tools: &restore-build-tools
    restore_cache:
      keys:
        - build-tool-cache-v2-{{ .Branch }}-{{ checksum "/tmp/pom_checksums.txt" }}-{{ checksum "project/build.properties" }}
        - build-tool-cache-v2-{{ .Branch }}-
        - build-tool-cache-v2-
  # This is a cache for the whole workspace. It's unique per CircleCI build and only used downstream of build-sbt.
  # Why not 'persist_to_workspace'? The deploy job depends on both build-maven and build-sbt. Both produce the same
  # files which causes CircleCI to fail restoring the workspace in deploy.
  # The caching key is unique per CircleCI build (of a single job) meaning anytime build-sbt is run or re-run, a new
  # cache key is used. When restoring we only match on the Git revision, such that we get the latest cached workspace.
  cache-workspace: &cache-workspace
    save_cache:
      key: workspace-cache-v1-{{ .Revision }}-{{ .BuildNum }}
      paths: [ /home/circleci/project ]
  restore-workspace: &restore-workspace
    restore_cache:
      keys:
        - workspace-cache-v1-{{ .Revision }}-
  cache-ivy: &cache-ivy2
    save_cache:
      key: ivy2-cache-v1-{{ .Branch }}-{{ checksum "/tmp/pom_checksums.txt" }}
      paths: [ ~/.ivy2/cache ]
  restore-ivy: &restore-ivy2
    restore_cache:
      keys:
        - ivy2-cache-v1-{{ .Branch }}-{{ checksum "/tmp/pom_checksums.txt" }}
        - ivy2-cache-v1-{{ .Branch }}-
        - ivy2-cache-v1-
  cache-sbt: &cache-sbt
    save_cache:
      key: sbt-cache-v1-{{ .Branch }}-{{ checksum "/tmp/pom_checksums.txt" }}-{{ checksum "project/build.properties" }}
      paths: [ ~/.sbt ]
  restore-sbt: &restore-sbt
    restore_cache:
      keys:
        - sbt-cache-v1-{{ .Branch }}-{{ checksum "/tmp/pom_checksums.txt" }}-{{ checksum "project/build.properties" }}
        - sbt-cache-v1-{{ .Branch }}-
        - sbt-cache-v1-
  checkout-code: &checkout-code
    run:
      name: Checkout code
      command: |
        # Copy of circle's checkout command with fix for fetching tags from
        # https://discuss.circleci.com/t/fetching-circle-tag-doesnt-seem-to-work/19014/2

        # Workaround old docker images with incorrect $HOME
        # check https://github.com/docker/docker/issues/2968 for details
        if [ "${HOME}" = "/" ]
        then
          export HOME=$(getent passwd $(id -un) | cut -d: -f6)
        fi

        mkdir -p ~/.ssh

        echo 'github.com ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAq2A7hRGmdnm9tUDbO9IDSwBK6TbQa+PXYPCPy6rbTrTtw7PHkccKrpp0yVhp5HdEIcKr6pLlVDBfOLX9QUsyCOV0wzfjIJNlGEYsdlLJizHhbn2mUjvSAHQqZETYP81eFzLQNnPHt4EVVUh7VfDESU84KezmD5QlWpXLmvU31/yMf+Se8xhHTvKSCZIFImWwoG6mbUoWf9nzpIoaSjB+weqqUUmpaaasXVal72J+UX2B+2RPW3RcT0eOzQgqlJL3RKrTJvdsjE3JEAvGq3lGHSZXy28G3skua2SmVi/w4yCE6gbODqnTWlg7+wC604ydGXA8VJiS5ap43JXiUFFAaQ==
        bitbucket.org ssh-rsa AAAAB3NzaC1yc2EAAAABIwAAAQEAubiN81eDcafrgMeLzaFPsw2kNvEcqTKl/VqLat/MaB33pZy0y3rJZtnqwR2qOOvbwKZYKiEO1O6VqNEBxKvJJelCq0dTXWT5pbO2gDXC6h6QDXCaHo6pOHGPUy+YBaGQRGuSusMEASYiWunYN0vCAI8QaXnWMXNMdFP3jHAJH0eDsoiGnLPBlBp4TNm6rYI74nMzgz3B9IikW4WVK+dc8KZJZWYjAuORU3jc1c/NPskD2ASinf8v3xnfXeukU0sJ5N6m5E8VLjObPEO+mN2t/FZTMZLiFqPWc/ALSqnMnnhwrNi2rbfg/rd/IpL8Le3pSBne8+seeFVBoGqzHM9yXw==
        ' >> ~/.ssh/known_hosts

        (umask 077; touch ~/.ssh/id_rsa)
        chmod 0600 ~/.ssh/id_rsa
        (cat <<EOF > ~/.ssh/id_rsa
        $CHECKOUT_KEY
        EOF
        )

        # use git+ssh instead of https
        git config --global url."ssh://git@github.com".insteadOf "https://github.com" || true

        if [ -e /home/circleci/project/.git ]
        then
          cd /home/circleci/project
          git remote set-url origin "$CIRCLE_REPOSITORY_URL" || true
        else
          mkdir -p /home/circleci/project
          cd /home/circleci/project
          git clone "$CIRCLE_REPOSITORY_URL" .
        fi

        if [ -n "$CIRCLE_TAG" ]
        then
          git fetch origin "+refs/tags/${CIRCLE_TAG}:refs/tags/${CIRCLE_TAG}"
        elif [ -n "$CIRCLE_PR_NUMBER" ]
        then
          git fetch origin "+${CIRCLE_BRANCH}/head:remotes/origin/${CIRCLE_BRANCH}"
        else
          git fetch origin "+${CIRCLE_BRANCH}:remotes/origin/${CIRCLE_BRANCH}"
        fi

        if [ -n "$CIRCLE_TAG" ]
        then
          git reset --hard "$CIRCLE_SHA1"
          git checkout -q "$CIRCLE_TAG"
        elif [ -n "$CIRCLE_BRANCH" ]
        then
          git reset --hard "$CIRCLE_SHA1"
          git checkout -q -B "$CIRCLE_BRANCH"
        fi

        git reset --hard "$CIRCLE_SHA1"

jobs:
  build-maven:
    <<: *defaults
    # Some part of the maven setup fails if there's no R, so we need to use the R image here
    docker:
      - image: palantirtechnologies/circle-spark-r:0.2.3
    steps:
      - *checkout-code
      - *create-pom-checksum-file
      - *restore-build-tools
      - restore_cache:
          keys:
            - m2-cache-v1-{{ .Branch }}-{{ checksum "/tmp/pom_checksums.txt" }}
            - m2-cache-v1-{{ .Branch }}-
            - m2-cache-v1-
      - run: ./build/mvn -DskipTests -Psparkr install
      # Get sbt to run trivially, ensures its launcher is downloaded under build/
      - run: ./build/sbt -h || true
      - *cache-build-tools
      - save_cache:
          key: m2-cache-v1-{{ .Branch }}-{{ checksum "/tmp/pom_checksums.txt" }}
          paths: [ ~/.m2/repository ]
      - persist_to_workspace:
          root: /home/circleci
          paths: [ project, .m2 ]

  build-spark-docker-gradle-plugin:
    <<: *defaults
    steps:
      - *checkout-code
      - restore_cache:
          keys:
            - gradle-cache-v1-{{ .Branch }}-{{ checksum "versions.props" }}-{{ checksum "build.gradle" }}-{{ checksum "gradle/wrapper/gradle-wrapper.properties" }}
            - gradle-cache-v1-{{ .Branch }}-
            - gradle-cache-v1-
      - run: ./gradlew --info --stacktrace check -x test
      - save_cache:
          key: gradle-cache-v1-{{ .Branch }}-{{ checksum "versions.props" }}-{{ checksum "build.gradle" }}-{{ checksum "gradle/wrapper/gradle-wrapper.properties" }}
          paths:
           - ~/.gradle/wrapper
           - ~/.gradle/caches
      - persist_to_workspace:
          root: /home/circleci
          paths: [ project, .gradle ]

  run-style-tests:
    <<: *test-defaults
    resource_class: medium+
    steps:
      - *restore-workspace
      - run:
          name: Run style tests
          command: dev/run-style-tests.py
          no_output_timeout: 20m

  run-build-tests:
    <<: *test-defaults
    resource_class: small
    steps:
      - attach_workspace: { at: /home/circleci }
      - run: |
          dev/run-build-tests.py | tee /tmp/run-build-tests.log
      - store_artifacts:
          path: /tmp/run-build-tests.log
          destination: run-build-tests.log

  build-sbt:
    <<: *defaults
    steps:
      - *checkout-code
      - *create-pom-checksum-file
      - *restore-build-tools
      - *restore-sbt
      - *restore-ivy2
      - run:
          name: Download all external dependencies for the test configuration (which extends compile) and ensure we update first
          command: dev/sbt test:externalDependencyClasspath oldDeps/test:externalDependencyClasspath
      - run: |
          dev/build-apache-spark.py | tee /tmp/build-apache-spark.log
      - store_artifacts:
          path: /tmp/heap.bin
      - store_artifacts:
          path: /tmp/build-apache-spark.log
          destination: build-apache-spark.log
      - *cache-ivy2
      - *cache-sbt
      - *cache-workspace

  run-spark-docker-gradle-plugin-tests:
    <<: *test-defaults
    resource_class: medium
    steps:
      - attach_workspace: { at: /home/circleci }
      - setup_remote_docker: { docker_layer_caching: true }
      - run: ./gradlew --info --parallel --continue --stacktrace test

  run-backcompat-tests:
    # depends on build-sbt
    <<: *defaults
    steps:
      - *restore-workspace
      - *create-pom-checksum-file
      - *restore-sbt
      - *restore-ivy2
      - run: |
          dev/run-backcompat-tests.py | tee /tmp/run-backcompat-tests.log
      - store_artifacts:
          path: /tmp/run-backcompat-tests.log
          destination: run-backcompat-tests.log


  run-python-tests:
    # depends on build-sbt, but we only need the assembly jars
    <<: *defaults
    docker:
      - image: palantirtechnologies/circle-spark-python:0.2.3
    parallelism: 2
    steps:
      - *restore-workspace
      - run: dev/run-python-tests.py
      - store_test_results:
          path: target/test-reports
      - store_artifacts:
          path: python/unit-tests.log


  run-r-tests:
    # depends on build-sbt, but we only need the assembly jars
    <<: *defaults
    docker:
      - image: palantirtechnologies/circle-spark-r:0.2.3
    steps:
      - *restore-workspace
      - run:
          name: Install SparkR
          command: R/install-dev.sh
      - run: dev/run-r-tests.py
      - store_test_results:
          path: target/R


  run-scala-tests:
    <<: *test-defaults
    # project/CirclePlugin.scala does its own test splitting in SBT based on CIRCLE_NODE_INDEX, CIRCLE_NODE_TOTAL
    parallelism: 12
    # Spark runs a lot of tests in parallel, we need 16 GB of RAM for this
    resource_class: xlarge
    steps:
      - *restore-workspace
      - *create-pom-checksum-file
      - *restore-sbt
      - *restore-ivy2
      - run:
          name: Before running tests, ensure we created the CIRCLE_TEST_REPORTS directory
          command: mkdir -p $CIRCLE_TEST_REPORTS
      - run:
          name: Run all tests
          command: ./dev/run-scala-tests.py | tee -a "/tmp/run-scala-tests.log"
          no_output_timeout: 20m
      - store_artifacts:
          path: /tmp/run-scala-tests.log
          destination: run-scala-tests.log
      - run:
          name: Collect unit tests
          command: mkdir -p /tmp/unit-tests && find . -name unit-tests.log -exec rsync -R {} /tmp/unit-tests/ \;
          when: always
      - store_artifacts:
          path: /tmp/unit-tests
      - store_artifacts:
          path: target/tests-by-bucket.json
          destination: tests-by-bucket.json
      - store_test_results:
          # TODO(dsanduleac): can we use $CIRCLE_TEST_RESULTS here?
          path: /tmp/circle-test-reports
      - run:
          name: Collect yarn integration test logs
          command: |
            shopt -s nullglob
            files=(resource-managers/yarn/target/./org.apache.spark.deploy.yarn.*/*-logDir-*)
            mkdir -p /tmp/yarn-tests
            if [[ ${#files[@]} != 0 ]]; then
              rsync -Rrm "${files[@]}" /tmp/yarn-tests/
            fi
          when: always
      - store_artifacts:
          path: /tmp/yarn-tests

  build-maven-versioned:
    <<: *defaults
    # Some part of the maven setup fails if there's no R, so we need to use the R image here
    docker:
      - image: palantirtechnologies/circle-spark-r:0.2.3
    steps:
      - attach_workspace: { at: /home/circleci }
      - run:
          command: dev/set_version_and_package.sh
      - persist_to_workspace:
          root: /home/circleci
          paths: [ project, .m2 ]

  deploy-gradle:
    <<: *defaults
    docker:
      - image: palantirtechnologies/circle-spark-r:0.2.3
    steps:
      - attach_workspace: { at: /home/circleci }
      - deploy:
          command: ./gradlew --parallel --continue --stacktrace --no-daemon bintrayUpload

  deploy:
    <<: *defaults
    # Some part of the maven setup fails if there's no R, so we need to use the R image here
    docker:
      - image: palantirtechnologies/circle-spark-r:0.2.3
    steps:
      - attach_workspace: { at: /home/circleci }
      - run: echo "user=$BINTRAY_USERNAME" > .credentials
      - run: echo "password=$BINTRAY_PASSWORD" >> .credentials
      - run: echo "realm=Bintray API Realm" >> .credentials
      - run: echo "host=api.bintray.com" >> .credentials
      - deploy:
          command: dev/publish.sh
      - store_artifacts:
          path: /tmp/make-dist.log
          destination: make-dist.log
      - store_artifacts:
          path: /tmp/publish_artifacts.log
          destination: publish_artifacts.log

workflows:
  version: 2
  build-test-deploy:
    jobs:
      - build-maven:
          <<: *all-branches-and-tags
      - run-style-tests:
          requires:
            - build-maven
          <<: *all-branches-and-tags
      - run-build-tests:
          requires:
            - build-maven
          <<: *all-branches-and-tags
      - build-sbt:
          <<: *all-branches-and-tags
      - build-spark-docker-gradle-plugin:
          <<: *all-branches-and-tags
      - run-backcompat-tests:
          requires:
            - build-sbt
          <<: *all-branches-and-tags
      - run-scala-tests:
          requires:
            - build-sbt
          <<: *all-branches-and-tags
      - run-spark-docker-gradle-plugin-tests:
          requires:
            - build-spark-docker-gradle-plugin
          <<: *all-branches-and-tags
      - run-python-tests:
          requires:
            - build-sbt
          <<: *all-branches-and-tags
      - run-r-tests:
          requires:
            - build-sbt
          <<: *all-branches-and-tags
      - build-maven-versioned:
          requires:
            - build-maven
          <<: *all-branches-and-tags
      - deploy-gradle:
          requires:
            - run-spark-docker-gradle-plugin-tests
          <<: *deployable-branches-and-tags
      - deploy:
          requires:
            - run-style-tests
            - run-build-tests
            - run-backcompat-tests
            - run-scala-tests
            - run-python-tests
            - run-r-tests
            - build-maven-versioned
          <<: *deployable-branches-and-tags
